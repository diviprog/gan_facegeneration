{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3939109,"sourceType":"datasetVersion","datasetId":2338285}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-21T10:42:00.496309Z","iopub.execute_input":"2024-08-21T10:42:00.497097Z","iopub.status.idle":"2024-08-21T10:42:00.521643Z","shell.execute_reply.started":"2024-08-21T10:42:00.497062Z","shell.execute_reply":"2024-08-21T10:42:00.520931Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os\nfrom PIL import Image\n\n# Set parameters\nIMG_SIZE = 64\nBATCH_SIZE = 128\nNOISE_DIM = 100\nEPOCHS = 1000\nBUFFER_SIZE = 60000","metadata":{"execution":{"iopub.status.busy":"2024-08-21T10:42:00.523070Z","iopub.execute_input":"2024-08-21T10:42:00.523354Z","iopub.status.idle":"2024-08-21T10:42:00.528376Z","shell.execute_reply.started":"2024-08-21T10:42:00.523330Z","shell.execute_reply":"2024-08-21T10:42:00.527331Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\ndef load_images_from_directory(data_dir, img_size):\n    all_images = []\n    for root, dirs, files in os.walk(data_dir):\n        for file in files:\n            if file.endswith(('.png', '.jpg', '.jpeg')):  # Add more image formats if needed\n                img_path = os.path.join(root, file)\n                img = Image.open(img_path).convert('RGB')\n                img = img.resize((img_size, img_size))\n                img = np.array(img)\n                all_images.append(img)\n    return np.array(all_images)\n\ndef preprocess_images(images):\n    images = images.astype('float32')\n    images = (images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n    return images\n\ndata_dir = '/kaggle/input/celebrity-face-image-dataset/Celebrity Faces Dataset'\nimages = load_images_from_directory(data_dir, IMG_SIZE)\nimages = preprocess_images(images)\ndataset = tf.data.Dataset.from_tensor_slices(images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T10:42:00.529652Z","iopub.execute_input":"2024-08-21T10:42:00.530339Z","iopub.status.idle":"2024-08-21T10:42:08.467507Z","shell.execute_reply.started":"2024-08-21T10:42:00.530309Z","shell.execute_reply":"2024-08-21T10:42:08.466548Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def build_generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(NOISE_DIM,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((8, 8, 256)))\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    \n    return model\n\ngenerator = build_generator()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T10:42:08.468763Z","iopub.execute_input":"2024-08-21T10:42:08.469111Z","iopub.status.idle":"2024-08-21T10:42:08.619173Z","shell.execute_reply.started":"2024-08-21T10:42:08.469079Z","shell.execute_reply":"2024-08-21T10:42:08.618340Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_discriminator():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[IMG_SIZE, IMG_SIZE, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model\n\ndiscriminator = build_discriminator()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T10:42:08.621698Z","iopub.execute_input":"2024-08-21T10:42:08.622286Z","iopub.status.idle":"2024-08-21T10:42:08.703966Z","shell.execute_reply.started":"2024-08-21T10:42:08.622237Z","shell.execute_reply":"2024-08-21T10:42:08.703004Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    return real_loss + fake_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T10:42:08.705123Z","iopub.execute_input":"2024-08-21T10:42:08.705452Z","iopub.status.idle":"2024-08-21T10:42:08.718623Z","shell.execute_reply.started":"2024-08-21T10:42:08.705425Z","shell.execute_reply":"2024-08-21T10:42:08.717845Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        for image_batch in dataset:\n            train_step(image_batch)\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch} completed')\n\ntrain(dataset, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T10:42:08.719791Z","iopub.execute_input":"2024-08-21T10:42:08.720391Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-08-21 10:42:11.628179: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1_2/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 completed\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n    predictions = model(test_input, training=False)\n    predictions = (predictions * 127.5 + 127.5).numpy().astype(np.uint8)\n    \n    for i in range(predictions.shape[0]):\n        img = Image.fromarray(predictions[i])\n        img.save(f'image_at_epoch_{epoch:04d}_{i}.png')\n\n# Example usage:\nseed = tf.random.normal([16, NOISE_DIM])\ngenerate_and_save_images(generator, 0, seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}